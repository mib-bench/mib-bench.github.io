<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MIB – Project Page</title>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap">
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<link href="fontawesome-6.6.0/css/all.css" rel="stylesheet">
    <style>
		.btn {
		    padding: 7px 15px;
			font-size: 20px;
		}
		
		mark {
			-webkit-animation: 3s highlight 1.5s 1 normal forwards;
			animation: 3s highlight 1.5s 1 normal forwards;
			background-color: none;
			background: linear-gradient(90deg, #f7f5bc 50%, rgba(255, 255, 255, 0) 50%);
			background-size: 200% 100%;
			background-position: 100% 0;
		}
		
		@-webkit-keyframes highlight {
		  to {
			background-position: 0 0;
		  }
		}

		@keyframes highlight {
		  to {
			background-position: 0 0;
		  }
		}
		
        body {
            font-family: 'Roboto', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #fff;
        }

        header {
            background-color: #1e1e1e;
            color: #fff;
            padding: 60px 0;
            text-align: center;
        }
		
		figure {
		  text-align: center; /* Centers the content inside the figure */
		  margin: 20px auto; /* Adds vertical space and centers the figure horizontally */
		}
		
		figcaption {
		  padding-top: 10px;
		  color: #555;
		  font-style: italic; /* Styling for the caption */
		}

<!--         header h1 {
			margin: 0;
			font-size: 60px;
			padding-bottom: 30px;
			padding-top: 20px;
			padding-left: 20px;
			padding-right: 20px;
        } -->
		
		header h1 {
            margin: 0;
            font-size: 100px;
			padding-bottom: 40px;
        }

        header h2 {
            margin: 10px 0 0;
            font-weight: 400;
        }
		
		header address a {
			font-size: 20px;
			color: #c78752;
		}
		
		header address {
			color: #c78752;
			text-align: center;
			margin: auto;
  			max-width: 1100px;
		}
		
		header address institute {
			color: #419F6C;
			font-size: 15px;
		}
		
		header address sup {
			color: #419F6C;
		}

        .container {
            width: 90%;
            max-width: 1200px;
            margin: 20px auto;
        }
		
        section {
            margin-bottom: 50px;
        }

        section h2 {
            font-size: 28px;
            border-left: 5px solid #4a9aac;
            padding-left: 10px;
            color: #333;
        }

        section p {
            font-size: 18px;
            line-height: 1.6;
            margin-top: 10px;
        }
		
		        @media (max-width: 768px) {
            header h1 {
                font-size: 32px;
            }

            header address a {
                font-size: 16px;
            }

            .container {
                width: 100%;
                padding: 0 15px;
            }

            .figure-container figure {
                flex: 1 1 100%;
            }

            .key-contributions, .materials, .causalvariable .circuit .motivation {
                padding: 15px;
            }

            section h2 {
                font-size: 20px;
            }
        }

        @media (max-width: 480px) {
            header h1 {
                font-size: 28px;
            }

            header address a {
                font-size: 14px;
            }

            .key-contributions, .materials, .causalvariable .circuit .motivation {
                padding: 10px;
            }

            section p {
                font-size: 14px;
            }
        }
		
		/* On larger screens (e.g., tablets, desktops) */
		@media (min-width: 768px) {
			img.float-figure {
				float: right;
				margin-left: 20px;
				margin-right: 0;
				width: 30%; /* Adjust based on your layout */
			}
		}

		/* On smaller screens (e.g., phones), reset to inline */
		@media (max-width: 767px) {
			img.float-figure {
				float: none;
				margin-left: auto;
				margin-right: auto;
				width: 100%; /* Full width inline */
			}
		}
		
		.figure-container {
		  display: grid;
		  grid-template-columns: 1fr 1fr; /* Creates two columns of equal width */
		  gap: 20px; /* Space between columns */
		}
		
		.subtitle {
		    font-size: 28px;
            border-left: 5px solid #1e1e1e;
            padding-left: 10px;
            color: #333;
		}

        .key-contributions, .materials, .causalvariable {
            background-color: #fff;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
			font-size: 18px;
        }
		
		.circuit, .motivation {
            background-color: #ededed;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
			font-size: 18px;
        }

        .key-contributions h3, .materials h3 .causalvariable h3 .circuit h3 .motivation h3 {
            font-size: 24px;
            color: #444;
        }

        .key-contributions ul, .materials ul .causalvariable ul .circuit ul .motivation ul {
            margin-left: 0px;
			font-size: 18px;
        }

        footer {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 5px 0;
        }

        footer p {
            margin: 0;
            font-size: 16px;
        }
		.card {
			position: relative;
			display: -webkit-box;
			display: -webkit-flex;
			display: -ms-flexbox;
			display: flex;
			-webkit-box-orient: vertical;
			-webkit-box-direction: normal;
			-webkit-flex-direction: column;
			-ms-flex-direction: column;
			flex-direction: column;
			background-color: #fff;
			border: 1px solid rgba(0, 0, 0, .125);
			border-radius: .25rem;
		.card-header {
			padding: .75rem 1.25rem;
			margin-bottom: 0;
			margin-top: 0;
			background-color: #f7f7f9;
			border-bottom: 1px solid rgba(0, 0, 0, .125);
			}
		.card-block {
			-webkit-box-flex: 1;
			-webkit-flex: 1 1 auto;
			-ms-flex: 1 1 auto;
			flex: 1 1 auto;
			padding: 1.25rem;
		}
		.img-inline {
		  vertical-align: middle;
		  max-width: 100%;
		  height: auto;
		}
    </style>
</head>
<body>

<header>
    <h1><strong>MIB</strong>: A <strong>M</strong>echanistic <strong>I</strong>nterpretability <strong>B</strong>enchmark</h1>
	<address>
	<a href="https://aaronmueller.github.io/" target="_blank">Aaron Mueller</a><sup><span style="color:#c78752">*</span>,1,2</sup>,
	<a href="https://atticusg.github.io/" target="_blank">Atticus Geiger</a><sup><span style="color:#c78752">*</span>,3</sup>,
	<a href="https://sarahwie.github.io/" target="_blank">Sarah Wiegreffe</a><sup><span style="color:#c78752">*</span>,4</sup>,
	<a href="https://il.linkedin.com/in/dana-arad" target="_blank">Dana Arad</a><sup>2</sup>,
	<a href="https://iarcuschin.com/" target="_blank">Iván Arcuschin</a><sup>5</sup>,
	<a href="https://x.com/adambelfki" target="_blank">Adam Belfki</a></a><sup>1</sup>,
	<a href="https://yiksiu-chan.github.io/" target="_blank">Yik Siu Chan</a><sup>6</sup>,
	<a href="https://github.com/JadenFiotto-Kaufman" target="_blank">Jaden Fiotto-Kaufman</a></a><sup>1</sup>,
	<a href="https://x.com/tal_haklay" target="_blank">Tal Haklay</a><sup>2</sup>,
	<a href="https://hannamw.github.io/" target="_blank">Michael Hanna</a><sup>7</sup>,
	<a href="https://scholar.google.com/citations?hl=en&user=zM_wp_MAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Jing Huang</a><sup>8</sup>,
	<a href="" target="_blank">Rohan Gupta</a><sup>5</sup>,
	<a href="https://yaniv.nikankin.com/" target="_blank">Yaniv Nikankin</a><sup>2</sup>,
	<a href="https://orgadhadas.github.io/" target="_blank">Hadas Orgad</a><sup>2</sup>,
	<a href="https://nix07.github.io/" target="_blank">Nikhil Prakash</a><sup>1</sup>,
	<a href="https://anja.re/" target="_blank">Anja Reusch</a><sup>2</sup>,
	<a href="https://x.com/arunasank?lang=en" target="_blank">Aruna Sankaranarayanan</a><sup>9</sup>,
	<a href="https://x.com/shunshao6" target="_blank">Shun Shao</a><sup>10</sup>,
	<a href="https://alestolfo.github.io/" target="_blank">Alessandro Stolfo</a><sup>11</sup>,
	<a href="https://mttk.github.io/" target="_blank">Martin Tutek</a><sup>2</sup>,
	<a href="https://amirzur.github.io/" target="_blank">Amir Zur</a><sup>3</sup>,
	<a href="https://baulab.info/" target="_blank">David Bau</a></a><sup>1</sup>,
	<a href="https://belinkov.com/" target="_blank">Yonatan Belinkov</a><sup>2</sup>
	 <br>
	<nobr><sup>1</sup><institute>Northeastern University</a></institute></nobr>&nbsp;&nbsp;&nbsp;
	<nobr><sup>2</sup><institute>Technion – IIT</a></institute></nobr>&nbsp;&nbsp;&nbsp;
	<nobr><sup>3</sup><institute>Pr(Ai)<sup>2</sup>R Group</a></institute></nobr>&nbsp;&nbsp;&nbsp;
	<nobr><sup>4</sup><institute>AI2</a></institute></nobr>&nbsp;&nbsp;&nbsp;
	<nobr><sup>5</sup><institute>Independent</a></institute></nobr>&nbsp;&nbsp;&nbsp;
	<nobr><sup>6</sup><institute>Brown University</a></institute></nobr>&nbsp;&nbsp;&nbsp;
	<nobr><sup>7</sup><institute>University of Amsterdam</a></institute></nobr>&nbsp;&nbsp;&nbsp;
	<nobr><sup>8</sup><institute>Stanford University</a></institute></nobr>&nbsp;&nbsp;&nbsp;
	<nobr><sup>9</sup><institute>Massachusetts Institute of Technology</a></institute></nobr>&nbsp;&nbsp;&nbsp;
	<nobr><sup>10</sup><institute>University of Cambridge</a></institute></nobr>&nbsp;&nbsp;&nbsp;
	<nobr><sup>11</sup><institute>ETH Zürich</a></institute></nobr>&nbsp;&nbsp;&nbsp;
	</address>
	<br>
	<a href="" target="_blank" class="btn" style="color: #fff; background-color: #c78752; border-color: #6d3b12;"><i class="ai ai-arxiv"></i> Paper</a>
	<a href="https://huggingface.co/collections/mib-bench/mib-datasets-67f55273612ec3067a42a56b" target="_blank" class="btn" style="color: #000; background-color: #fff; border-color: #c1c1c1;"><i class="fas fa-database"></i> Data</a>
    <a href="https://github.com/aaronmueller/MIB" target="_blank" class="btn" style="color: #fff; background-color: #3a4149; border-color: #000000;"><i class="fab fa-github"></i> Code</a>
	<a href="https://huggingface.co/spaces/mib-bench/leaderboard" target="_blank" class="btn" style="color: #fff; background-color: #419F6C; border-color: #204e35;"><i class="far fa-chart-bar"></i> Leaderboard</a>
</header>


<div class="container">

	<figure style="padding-top: 50px; padding-bottom:100px">
		<img src="img/mib_logo.png" class="img-inline" width="95%">
	</figure>

	<h2 class="subtitle">Abstract</h2>
    <section>
        <p>
Progress in mechanistic interpretability has been rapid, but there exist few standards for comparing methods. How can we know whether new methods achieve real improvements? In pursuit of meaningful and lasting evaluation standards, we propose <strong>MIB</strong>, a <strong>M</strong>echanistic <strong>I</strong>nterpretability <strong>B</strong>enchmark with two tracks spanning four tasks and five models. <mark><strong>MIB</strong> favors methods that precisely and concisely recover relevant causal pathways or specific causal variables in neural language models.</mark> The circuit localization track compares methods that locate the model components—and connections between them—that are most important for performing a task (e.g., attribution patching or information flow routes). The causal variable localization track compares methods that featurize a hidden vector (e.g., sparse autoencoders or distributed alignment search) and locate mediators of a causal variable relevant to the task. Using <strong>MIB</strong>, we find that attribution and mask optimization methods perform best on circuit localization. We also find that supervised methods for causal variable localization outperform unsupervised methods, and both outperform the simplest baseline. These findings illustrate that <mark><strong>MIB</strong> enables meaningful comparisons of MI methods, and increases our confidence that there has been real progress in the field</mark>.
        </p>
    </section>

	<h2 class="subtitle">Key Contributions</h2>
    <section class="key-contributions">
        <ul>
		<li><strong>New metrics:</strong> Two integrated faithfulness metrics for evaluating circuit discovery methods</li>
		<li><strong>New model:</strong> A model with a ground-truth circuit</li>
		<li><strong>Standard datasets and counterfactuals:</strong> Tasks and causal variables of varying difficulties and required reasoning types</li>
		<li><strong>Novel scientific insights:</strong> Edge-level circuits outperform node-level; attribution and mask learning methods are best for circuit discovery; DAS outperforms mask learning and SAEs</li>
        </ul>
    </section>
	
	<h2 class="subtitle">Motivation</h2>
    <section class="motivation">
		Mechanistic interpretability (MI) methods allow us to understand why language models (LMs) behave the way they do. MI methods
		have been proliferating quickly, but it's difficult to compare the efficacy of MI methods. How can we know whether true methods
		are producing real advancements over prior work? We propose <strong>MIB</strong> as a stable standard.

		<h3>Types of MI Methods</h3>
		We view most MI methods as performing either <em>localization</em> or <em>featurization</em> (or both). We split these two functions into
		two tracks: the <strong>circuit localization track</strong>, and the <strong>causal variable track</strong>.
    </section>
	
	<h2 class="subtitle">Materials</h2>
    <section class="materials">
		<h3>Data</h3>
		Both tracks evaluate across four tasks. These are selected to represent various reasoning types, difficulty levels, and answer formats.
		<ul>
			<li>Indirect Object Identification (IOI)</li>
			<li>Multiple-choice Question Answering (MCQA)</li>
			<li>Arithmetic</li>
			<li>AI2 Reasoning Challenge (ARC)</li>
		</ul>

		Two of these tasks (IOI and Arithmetic) were chosen because they have been extensively studied. The others (MCQA and ARC) were chosen because
		they have not.

		<h3>Models</h3>
		We include models of diverse capability levels and sizes:
		<ul>
			<li>GPT-2 Small</li>
			<li>Qwen-2.5 (0.5B)</li>
			<li>Gemma-2 (2B)</li>
			<li>Llama-3.1 (8B)</li>
		</ul>
    </section>

	<h2 class="subtitle">Circuit Localization Track</h2>
	<section class="circuit">
		<figure>
			<img src="img/circuit_track.png" class="img-inline" style="width:90%; min-width: 150px; height:auto; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);">
		</figure>
		Given a task, a <strong>circuit</strong> is the subset of the computation graph that performs the task.

		<h3>Metrics</h3>
		Past circuit discovery work often uses <strong>faithfulness</strong>. This is good for measuring the quality of a single circuit, but how do we measure the quality
		of a circuit discovery method?
		<br><br>
		Furthermore, people often mean one of two things by this: (i) the subgraph that is responsible for performing the task well, or (ii) the
		smallest subgraph that replicates the model's behavior (including its failures).
		<br><br>
		Thus, <mark>we propose two metrics: the <strong>integrated circuit performance ratio</strong> (CPR; higher is better), and the <strong>integrated circuit-model difference</strong> (CMD; 0 is best).</mark>
		CPR is basically the area under the faithfulness curve at many circuit sizes. CMD is the area between the faithfulness curve and 1, where 1 indicates that the circuit and model
		have the exact same task behavior (with respect to what is being measured).
		<figure>
			<img src="img/f_curves.png" class="img-inline" style="width:40%; min-width: 150px; height:auto; border-radius:5px 5px 5px 5px; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);">
			<figcaption>Illustration of CPR (area under the faithfulness curve) and CMD (area between the faithfulness curve and 1).</figcaption>
		</figure>
		<br><br>
		An issue with faithfulness is that it's not clear what the lower or upper bounds are. Thus, we include a fifth model for this track: an <em>InterpBench</em> model. This is a model
		that we train to contain a known ground-truth circuit. Because we know what the edges are, we can compute the AUROC over the edges at many circuit sizes.
		
		<h3>Baselines</h3>
		We evaluate a variety of methods, including:
		<ul>
			<li>Activation patching</li>
			<li>Gradient attribution methods</li>
			<li>Mask learning methods</li>
			<li>Information flow routes</li>
			<li>Edge-level and node-level circuit discovery</li>
		</ul>

		<h3>Results</h3>
		<figure>
			<img src="img/circuit_results.png" class="img-inline" style="width:85%; min-width: 150px; height:auto; border-radius:5px 5px 5px 5px; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);">
			<figcaption>CMD scores (closer to 0 is better), and AUROC scores (for InterpBench only; higher is better).</figcaption>
		</figure>
		<img src="img/arrow.gif" class="img-inline" style="height:50px"> Attribution patching with integrated gradients (*AP-IG) outperforms attribution patching (*AP), and most other
		methods.<br>
		<img src="img/arrow.gif" class="img-inline" style="height:50px"> Edge-level circuits (E*) outperform node-level circuits (A*).<br>
		<img src="img/arrow.gif" class="img-inline" style="height:50px"> Patching with activations from counterfactual inputs (CF) outperforms
		other common patching methods.<br>
		<img src="img/arrow.gif" class="img-inline" style="height:50px"> UGS, a mask-learning method, performs well.<br>
	</section>

	<h2 class="subtitle">Causal Variable Localization Track</h2>
	<section class="causalvariable">
		<figure>
			<img src="img/causalvariable_track.png" class="img-inline" style="width:90%; min-width: 150px; height:auto; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);">
		</figure>
		In this track, the goal is to align model representations with specific known causal variables.
		<h3>Metrics</h3>
		We want to evaluate the quality of a <strong>featurizer</strong>, a transformation of the activations that
		makes it easier to isolate the desired causal variable. For this, we typically use <strong>interchange intervention accuracy</strong>.

		<figure>
			<img src="img/causalvariable_alignment.png" class="img-inline" style="width:50%; min-width: 150px; height:auto; border-radius:5px 5px 5px 5px; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);">
			<figcaption>Example of causal variable alignment. In the arithmetic task, one could hypothesize a carry-the-one variable.
				        We align this variable with model activations (featurized by F), and measure whether interventions to these features
					    result in the expected change in model behavior.</figcaption>
		</figure>

		<h3>Baselines</h3>
		We evaluate a mixture of supervised and unsupervised, as well as parametric and non-parametric, methods.
		<ul>
			<li>Distributed alignment search (DAS)</li>
			<li>Differentiable binary masks (DBM)</li>
			<li>Sparse autoencoders (SAE)</li>
			<li>Principal component analysis (PCA)</li>
		</ul>
		As a naive baseline, we compare to no featurizer (i.e., the full untransformed vector).

		<h3>Results</h3>
		<figure>
			<img src="img/causalvariable_results.png" class="img-inline" style="width:80%; min-width: 150px; height:auto; border-radius:5px 5px 5px 5px; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);">
			<figcaption>Interchange intervention accuracies (higher is better). For IOI, we instead measure the error between a hypothesized variable value and
				the model's output logits, so lower is better.
			</figcaption>
		</figure>
		<img src="img/arrow.gif" class="img-inline" style="height:50px"> DAS performs best.<br>
		<img src="img/arrow.gif" class="img-inline" style="height:50px"> SAEs are high-variance: sometimes they approach the performance of the best methods, but sometimes approach that of the worst.<br>
		<img src="img/arrow.gif" class="img-inline" style="height:50px"> PCA performs poorly.<br>
		<img src="img/arrow.gif" class="img-inline" style="height:50px"> In general, non-basis-aligned features perform better than basis-aligned features.<br>
	</section>

	<h2 class="subtitle">How to cite</h2>

	<div class="card">
	<h3 class="card-header">bibliography</h3>
	<div class="card-block">
	<p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect">
	Aaron Mueller*, Atticus Geiger*, Sarah Wiegreffe*, Dana Arad, Iván Arcuschin, Adam Belfki, Yik Siu Chan, Jaden Fiotto-Kaufman, Tal Haklay, Michael Hanna, Jing Huang, Rohan Gupta, Yaniv Nikankin, Hadas Orgad, Nikhil Prakash, Anja Reusch, Aruna Sankaranarayanan, Shun Shao, Alessandro Stolfo, Martin Tutek, Amir Zur, David Bau, Yonatan Belinkov, “<em>MIB: A Mechanistic Interpretability Benchmark</em>”.
	</p>
	</div>
	<h3 class="card-header">bibtex</h3>
	<div class="card-block">
	<pre class="card-text clickselect">
@article{mib-2025,
	title = {{MIB}: A Mechanistic Interpretability Benchmark},
	author = {Aaron Mueller and Atticus Geiger and Sarah Wiegreffe and Dana Arad and Iv{\'a}n Arcuschin and Adam Belfki and Yik Siu Chan and Jaden Fiotto-Kaufman and Tal Haklay and Michael Hanna and Jing Huang and Rohan Gupta and Yaniv Nikankin and Hadas Orgad and Nikhil Prakash and Anja Reusch and Aruna Sankaranarayanan and Shun Shao and Alessandro Stolfo and Martin Tutek and Amir Zur and David Bau and Yonatan Belinkov},
	year = {2025},
	note = {To appear},
	journal = {arXiv preprint}
}</pre>
	</div>
	</div>
</div>


<footer>
    <p>Created by Aaron Mueller and Hadas Orgad | 2025</p>
</footer>

</body>
</html>